import {
    Box,
    Card,
    Image,
    Heading,
    Text,
    Flex,
    Link
} from 'rebass';

import {Helmet} from "react-helmet";

export const staticSiteSetup = "https://docs.aws.amazon.com/AmazonS3/latest/userguide/HostingWebsiteOnS3Setup.html"

export const cloudFrontSetup = "https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/"

export const Repo = "https://github.com/arthurdayton116/blog-site"

export const Post1 = "https://arthurdayton.com/post/1/"

export const clientSideRoute = "https://create-react-app.dev/docs/deployment#serving-apps-with-client-side-routing"

export const goodExp = "https://wolovim.medium.com/deploying-create-react-app-to-s3-or-cloudfront-48dae4ce0af"

export const prodBuild = "https://create-react-app.dev/docs/production-build"

export const gaTerraform = "https://github.com/hashicorp/setup-terraform"

<!--  https://webcode.tools/generators/open-graph/article -->

<Helmet>
<meta property="og:type" content="article"/>
<meta name="title" property="og:title" content="Serverless Web Application Deployement on AWS"/>
<meta property="og:url" content="https://arthurdayton.com/post/6/"/>
<meta name="image" property="og:image" content="https://arthurdayton.com/images/6/preview.png"/>
<meta name="description" property="og:description" content="This post shows the code for deploying a serverless application to AWS using Terraform, React, GraphQL and GitHub Actions."/>
<meta name="author" property="article:author" content="Arthur Dayton"/>
<meta name="published_time" property="article:published_time" content="2022-02-16"/>
<meta property="article:tag" content="Terraform"/>
<meta property="article:tag" content="AWS"/>
<meta property="article:tag" content="React"/>
<meta property="article:tag" content="Cloudfront"/>
<meta property="article:tag" content="Route53"/>
<meta property="article:tag" content="GraphQL"/>
<meta property="article:tag" content="ACM"/>
<meta property="article:tag" content="GitHub Actions"/>
</Helmet>

#### Using GitHub Actions to continuously deploy a Serverless blog site

In this post I am going to cover how to use <a href="https://docs.github.com/en/actions" target="_blank">GitHub Actions</a> to continuously build and deploy a serverless web application (this site) using <a href="https://www.terraform.io/" target="_blank">Terraform</a>, <a href="https://reactjs.org/" target="_blank">React</a>, <a href="https://graphql.org/" target="_blank">GraphQL</a>, and several AWS services including <a href="https://aws.amazon.com/lambda/" target="_blank">Lambda</a>, <a href="https://aws.amazon.com/api-gateway/" target="_blank">API Gateway</a>, <a href="https://aws.amazon.com/s3/" target="_blank">S3</a>, <a href="https://aws.amazon.com/sns/" target="_blank">SNS</a>, <a href="https://aws.amazon.com/route53/" target="_blank">Route53</a>, <a href="https://aws.amazon.com/certificate-manager/" target="_blank">Certificate Manager</a> and <a href="https://aws.amazon.com/cloudfront/" target="_blank">CloudFront</a>.

<br/>

<details open style={{paddingLeft: props.theme.detail.paddingLeft}}>
<summary><strong>Diagram of what I am implementing:</strong></summary>


<Image p={4} verticalAlign='middle' src="/images/6/ArchitectureDiagram.png" />

</details>

<br/>

To set this up yourself you need an <a href="https://aws.amazon.com/account/" target="_blank">AWS account</a>, a <a href="https://cloud.hashicorp.com/products/terraform" target="_blank">Terraform Cloud account</a>, a <a href="https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/domain-register.html" target="_blank">custom domain</a>, basic React skills, basic Terraform skills, a working knowledge of GitHub Actions and of course the requisite amount of OCD needed to figure out the stuff I almost certainly left out (I wouldn't want to deny you the sweet pain of experience).

<br/>

I started with the <a href="https://create-react-app.dev/" target="_blank">Create React App</a> and built off of it <a href="https://arthurdayton.com/post/1/" target="_blank">How to set up a Git Hub Pages Blog Site Using React and MDX</a>.  The approach for using S3 to serve the static site assets is essentially the same as using GitHub pages but when you start to think about putting an API on the back end I think it's "easier" to use AWS.

<br/>

<details open style={{paddingLeft: props.theme.detail.paddingLeft}}><summary><strong>A little background reading from around the internet:</strong></summary>

<br/>
<br/>

<Link p={4} href={staticSiteSetup} target="_blank">S3 Static Site Setup</Link>

<br/>

S3 (Simple Storage Service) is definitely the <a href="https://en.wikipedia.org/wiki/Amazon_S3" target="_blank">the OG of AWS</a> .  It's cheap (It currrently costs me less than 5 cents a month), easy to use(<a href="https://threatpost.com/sega-security-aws-s3-exposed-steam/177352/" target="_blank">to easy?</a>), and pretty seamlessly integrates with lots of other AWS resources, such as Cloudfront.

<br/>

<Link p={4} href={cloudFrontSetup} target="_blank">CloudFront Setup</Link>

<br/>

Cloudfront is a CDN which allows users to cache content around the world and automatically integrates with S3 and Route53 so we can easily map our domain to our content.

<br/>

<Link p={4} href={goodExp} target="_blank">This post does a great job of explaining how to integrate S3 and Cloudfront</Link>

<br/>

</details>

<br/>
<br/>


<h2>Building out components of architecture diagram</h2>


To build out my architecture diagram above I use Terraform to define my AWS resources, the AWS CLI to push my React code to my S3 bucket, and a GitHub Actions pipeline to deploy whenever I push changes to the main branch of my repository.  My API is Apollo GraphQL (<a href="https://www.apollographql.com/" target="_blank">‚ù§Ô∏è‚Äçüî•</a>) for getting and submitting comments.  It's running as a Lambda function (<a href="https://www.apollographql.com/docs/apollo-server/deployment/lambda/" target="_blank">ü§Ø</a>) and using DynamoDB as a data store so that this site is essentially free to deploy and will autoscale to any degree of usage.

<br/>

I could have easily used all AWS tools to do this, but I think there is good reason to stick with some tools that are Cloud agnostic as it becomes clearer every day that the enterprise cloud provider battle is going to work out like the enterprise database or development language battles (everybody is going to have pretty much everything).

<br/>

<Link p={4} href={Repo} target="_blank">Code Here</Link>

<br/>
<br/>

<!-- ######################### Hosted Zone ################################## -->

<details style={{paddingLeft: props.theme.detail.paddingLeft}}>
<summary><h2 style={{display: 'inline'}}>Hosted Zone</h2>


If I want to reach the world then I'm going to need a domain and it will need to support SSL/TLS certificates so I'm using Route53 and ACM (Amazon Certificate Manager) to purchase and renew both.  My hosted zone in Route53 will route traffic to my Cloudfront distributions for my React site and my GraphQL endpoint.  I will need to create those Cloudfront distributions of course, which I show below. As I go along, I will be creating Terraform files for the creation of all resources so that I can easily create and destroy on demand as well as make the process of deploying my site automated and reusable.
</summary>
<br/>

<details style={{paddingLeft: props.theme.detail.paddingLeft}}>
    <summary><strong>Basic Terraform code looks like this:</strong></summary>

```json
// hosted zone
resource "aws_route53_zone" "main" {
  name = local.bucket_name
}

// gmail record
resource "aws_route53_record" "gmail_mx" {
  zone_id = aws_route53_zone.main.zone_id
  name    = local.bucket_name
  type    = "MX"

  records = [
    "1  ASPMX.L.GOOGLE.COM",
    "5  ALT1.ASPMX.L.GOOGLE.COM",
    "5  ALT2.ASPMX.L.GOOGLE.COM",
    "10 ALT3.ASPMX.L.GOOGLE.COM",
    "10 ALT4.ASPMX.L.GOOGLE.COM",
  ]

  ttl = "300"
}
```
</details>

<br/>

The most important thing to remember, especially  if you are switching what your domain points at, is that when you are sure it's not DNS it's probably DNS.

This shows my hosted zone in the AWS Console:

<Image p={4} verticalAlign='middle' src="/images/6/hostedzone.png" />

These are the records pointing to my cloudfront distributions:

<Image p={4} verticalAlign='middle' src="/images/6/hostedzone2.png" />

This shows the record detail and how to point to a Cloudfront distribution (I use Terrraform to create these when I deploy to Cloudfront below):

<Image p={4} verticalAlign='middle' src="/images/6/hzrecord.png" />
<br/>
<br/>


</details>

<br/>

<!-- ######################### Certificate ################################## -->

<details style={{paddingLeft: props.theme.detail.paddingLeft}}>
<summary><h2 style={{display: 'inline'}}>Certificate</h2>

Using ACM you can create a certificate for your domain and have it automatically validate by creating records in your hosted zone
</summary>

<br/>

This shows the certificate status and domain validation status in the ACM panel:

<Image p={4} verticalAlign='middle' src="/images/6/acm2.png" />

This shows the records created by the validation process (we can use Terraform for this as well):
<Image p={4} verticalAlign='middle' src="/images/6/certrecord.png" />
<br/>
<br/>

<details style={{paddingLeft: props.theme.detail.paddingLeft}}>
    <summary><strong>Terraform code:</strong></summary>

```json
//domain certificate
resource "aws_acm_certificate" "a" {
  domain_name               = local.bucket_name
  subject_alternative_names = ["www.${local.bucket_name}"]
  validation_method         = "DNS"
}

// validate that we have control over domain so our certificate is trusted
resource "aws_acm_certificate_validation" "example" {
  certificate_arn         = aws_acm_certificate.a.arn
  // loop route records which are the domain name and subject_alternative_names from aws_acm_certificate resource
  validation_record_fqdns = [for record in aws_route53_record.example : record.fqdn]
}


resource "aws_route53_record" "example" {
  for_each = {
    for dvo in aws_acm_certificate.a.domain_validation_options : dvo.domain_name => {
      name    = dvo.resource_record_name
      record  = dvo.resource_record_value
      type    = dvo.resource_record_type
      zone_id = aws_route53_zone.main.zone_id
    }
  }

  allow_overwrite = true
  name            = each.value.name
  records         = [each.value.record]
  ttl             = 60
  type            = each.value.type
  zone_id         = each.value.zone_id
}

// graphql certificate
resource "aws_acm_certificate" "graphql" {
  domain_name               = local.bucket_name
  subject_alternative_names = ["graphql.${local.bucket_name}"]
  validation_method         = "DNS"
}

resource "aws_acm_certificate_validation" "graphql" {
  certificate_arn         = aws_acm_certificate.graphql.arn
  validation_record_fqdns = [for record in aws_route53_record.graphql : record.fqdn]
}


resource "aws_route53_record" "graphql" {
  for_each = {
    for dvo in aws_acm_certificate.graphql.domain_validation_options : dvo.domain_name => {
      name    = dvo.resource_record_name
      record  = dvo.resource_record_value
      type    = dvo.resource_record_type
      zone_id = aws_route53_zone.main.zone_id
    }
  }

  allow_overwrite = true
  name            = each.value.name
  records         = [each.value.record]
  ttl             = 60
  type            = each.value.type
  zone_id         = each.value.zone_id
}

```
</details>
</details>

<br/>

<!-- ######################### Cloudfront ################################## -->

<details style={{paddingLeft: props.theme.detail.paddingLeft}}>
<summary><h2 style={{display: 'inline'}}>Cloudfront</h2>

After creating a Cloudfront distribution for my S3 bucket, that I set as an origin, I can point the name records in Route53 at Cloudfront which I showed in the Hosted Zone section above.  As with everything else we can automate and manage these with Terraform.
</summary>

<br/>

This shows my distribution for site assets:

<Image p={4} verticalAlign='middle' src="/images/6/cloudfront.png" />
<Image p={4} verticalAlign='middle' src="/images/6/cloudfront2.png" />

This shows the configured S3 bucket origin where my React build artifacts are pushed from the GitHub Actions pipeline that runs when I commit to my repository:

<Image p={4} verticalAlign='middle' src="/images/6/cloudfront3.png" />
<br/>
<br/>

<details style={{paddingLeft: props.theme.detail.paddingLeft}}>
    <summary><strong>Terraform code:</strong></summary>

```json
locals {
  s3_origin_id = "${local.resource_prefix}_${local.bucket_name}_a"
}

resource "aws_cloudfront_distribution" "s3_distribution_a" {
  origin {
    domain_name = local.s3_web_endpoint
    origin_id   = local.s3_origin_id

    // https://github.com/hashicorp/terraform-provider-aws/issues/7847
    custom_origin_config {
      http_port              = "80"
      https_port             = "443"
      origin_protocol_policy = "http-only"
      origin_ssl_protocols   = ["TLSv1", "TLSv1.1", "TLSv1.2"]
    }
  }

  enabled         = true
  is_ipv6_enabled = true
  //  comment             = ""
  default_root_object = "index.html"

  logging_config {
    include_cookies = false
    bucket          = local.log_bucket_name
    prefix          = local.resource_prefix
  }

  aliases = [local.bucket_name, local.alt_name]

  default_cache_behavior {
    allowed_methods  = ["DELETE", "GET", "HEAD", "OPTIONS", "PATCH", "POST", "PUT"]
    cached_methods   = ["GET", "HEAD"]
    target_origin_id = local.s3_origin_id

    forwarded_values {
      query_string = false

      cookies {
        forward = "none"
      }
    }

    viewer_protocol_policy = "https-only"
    min_ttl                = 0
    default_ttl            = 3600
    max_ttl                = 86400
  }

  price_class = "PriceClass_All"

  restrictions {

    geo_restriction {
      restriction_type = "none"
    }
  }

  tags = merge(
    local.base_tags,
    {
      Name = "${local.resource_prefix}-${local.bucket_name}"
    },
  )

  viewer_certificate {
    acm_certificate_arn = local.cert_arn
    ssl_support_method  = "sni-only"
  }
}

// Create subdomain A record pointing to Cloudfront distribution
resource "aws_route53_record" "www" {
  zone_id = local.zone_id
  name    = "www.${local.bucket_name}"
  type    = "A"

  alias {
    name                   = aws_cloudfront_distribution.s3_distribution_a.domain_name
    zone_id                = aws_cloudfront_distribution.s3_distribution_a.hosted_zone_id
    evaluate_target_health = false
  }
}

// Create domain A record pointing to Cloudfront distribution
resource "aws_route53_record" "a" {
  zone_id = local.zone_id
  name    = local.bucket_name
  type    = "A"

  alias {
    name                   = aws_cloudfront_distribution.s3_distribution_a.domain_name
    zone_id                = aws_cloudfront_distribution.s3_distribution_a.hosted_zone_id
    evaluate_target_health = false
  }
}
```
</details>
</details>

<br/>


<!-- ######################### Buckets ################################## -->

<details style={{paddingLeft: props.theme.detail.paddingLeft}}>
<summary><h2 style={{display: 'inline'}}>S3 Buckets</h2>

For this site I'm using 2 S3 buckets.  One for the site assets that we will use as a source for our CloudFront distribution and one for logs and Athena query results.

</summary>

<br/>

This shows the buckets:

<Image p={4} verticalAlign='middle' src="/images/6/S3list.png" />

This shows the site assets from React build:

<Image p={4} verticalAlign='middle' src="/images/6/S3assets.png" />

This shows relevant properties for site assets bucket:

<Image p={4} verticalAlign='middle' src="/images/6/S3domain.png" />
<Image p={4} verticalAlign='middle' src="/images/6/S3www.png" />

<details style={{paddingLeft: props.theme.detail.paddingLeft}}>
    <summary><strong>Terraform code:</strong></summary>

```json
// root domain bucket and policy

// Clean up bucket for destroy aws s3 rm s3://bucketname --recursive
resource "aws_s3_bucket" "a" {
  bucket = local.bucket_name

  // For a React site the index needs to be error document as well and React needs to handle errors
  website {
    index_document = "index.html"
    error_document = "index.html"
  }

  logging {
    target_bucket = aws_s3_bucket.log.id
    target_prefix = "s3-log/"
  }
  force_destroy = true
  tags = merge(
    local.base_tags,
    {
      Name      = "${local.resource_prefix}-${local.bucket_name}"
      directory = basename(path.cwd)
    },
  )
}

resource "aws_s3_bucket" "log" {
  bucket = "logs-${local.bucket_name}"

  force_destroy = true

  grant {
    type        = "Group"
    permissions = ["READ_ACP", "WRITE"]
    uri         = "http://acs.amazonaws.com/groups/s3/LogDelivery"
  }

  //CloudFront
  // Permissions required to configure standard logging and to access your log files
  // https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html
  grant {
    id          = "c4c1ede66af53448b93c283ce9448c4ba468c9432aa01d700d3878632f77d2d0"
    permissions = ["FULL_CONTROL", ]
    type        = "CanonicalUser"
  }

  grant {
    id          = data.aws_canonical_user_id.current_user.id
    type        = "CanonicalUser"
    permissions = ["FULL_CONTROL"]
  }

  tags = merge(
    local.base_tags,
    {
      Name = "${local.resource_prefix}-logs-${local.bucket_name}"
    },
  )
}

resource "aws_s3_bucket_object" "log_query_folder" {
  bucket = aws_s3_bucket.log.id
  acl    = "private"
  key    = "queryResults/"
  source = "/dev/null"
}
// This ensures that bucket content can be read publicly
// I like setting it up this way so it's easier to test changes
// Check here for different setups  - https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/
// Policy limits to specific bucket
resource "aws_s3_bucket_policy" "a" {
  bucket = aws_s3_bucket.a.id

  policy = jsonencode({
    Version = "2012-10-17"
    Id      = "${local.resource_prefix}-${local.bucket_name}-policy"
    Statement = [
      {
        Sid       = "PublicRead"
        Effect    = "Allow"
        Principal = "*"
        Action = [
          "s3:GetObject",
          "s3:GetObjectVersion"
        ]
        Resource = [
          aws_s3_bucket.a.arn,
          "${aws_s3_bucket.a.arn}/*",
        ]
      },
    ]
  })
}

```

</details>
</details>

<br/>

<!-- ######################### Github Actions ################################## -->

<details style={{paddingLeft: props.theme.detail.paddingLeft}}>
<summary><h2 style={{display: 'inline'}}>GitHub Actions</h2>

For my continuous integration I am using GitHub.  Right now I am using the main branch and not a PR flow because it's just me and I can make it work with local dev and GitHub actions.  In a context any larger than myself I would set up a separate dev domain either within a VPC or locked down by IP address.  GitHub Actions is pretty awesome and it's been exciting watching it mature towards being an Enterprise ready tool.  Care and feeding of architecture is painful and having this capability delivered right to my GitHub repository is awesome!  If you've used Azure Devops (lately) then it will look and feel very familiar.

</summary>

<h3>Environment</h3>
First thing I needed to do was set up an environment in Actions so I can store secrets:

<Image p={4} verticalAlign='middle' src="/images/6/actionsenv.png" />

<h3>Actions Workflow</h3>

Then I can set up the yaml file that will define my workflow that runs whenever I commit code.  GitHub has a default location for the file:

<Image p={4} verticalAlign='middle' src="/images/6/actionslocation.png" />

I reference my environment and then I can pull secrets into my build job runs securely:

<Image p={4} verticalAlign='middle' src="/images/6/yaml1.png" />

Now I can create all the steps needed to build this application from top to bottom:

<h4>Set up dynamodb container for automated tests</h4>
<Image p={4} verticalAlign='middle' src="/images/6/yaml2.png" />

<h4>Check out previous commit so we can see what files have changed and only run certain steps</h4>
<Image p={4} verticalAlign='middle' src="/images/6/yaml3.png" />
<Image p={4} verticalAlign='middle' src="/images/6/yaml4.png" />
<Image p={4} verticalAlign='middle' src="/images/6/yaml5.png" />

<h4>Set up Terraform</h4>
<Image p={4} verticalAlign='middle' src="/images/6/yaml6.png" />

<h4>Run Terraform apply if files have changed and set and environment variables needed for following steps</h4>
<Image p={4} verticalAlign='middle' src="/images/6/yaml7.png" />

<h4>Build graphql code, zip for Lambda and create Lambda</h4>
<Image p={4} verticalAlign='middle' src="/images/6/yaml8.png" />

<details style={{paddingLeft: props.theme.detail.paddingLeft}}>
    <summary><strong>API Gateway Terraform:</strong></summary>

```json
# Create an API endpoint for graphql lambda
resource "aws_api_gateway_rest_api" "gql" {
  name        = "BlogGraphql"
  description = "Terraform Serverless GraphQL Example"
}

resource "aws_api_gateway_account" "gql" {
  cloudwatch_role_arn = aws_iam_role.cloudwatch.arn
}

resource "aws_iam_role" "cloudwatch" {
  name = "${local.resource_prefix}-api_gateway_cloudwatch_global_gql"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    "Statement" : [
      {
        Sid : "",
        Effect : "Allow",
        Principal : {
          Service : "apigateway.amazonaws.com"
        },
        Action : "sts:AssumeRole"
      }
    ]
  })
}

resource "aws_iam_role_policy_attachment" "cloudwatch_gql" {
  policy_arn = "arn:aws:iam::aws:policy/service-role/AmazonAPIGatewayPushToCloudWatchLogs"
  role       = aws_iam_role.cloudwatch.name
}

// Creates method in api gateway for graphql
// https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-lambda-proxy-integrations.html
resource "aws_api_gateway_resource" "gql_method" {
  rest_api_id = aws_api_gateway_rest_api.gql.id
  parent_id   = aws_api_gateway_rest_api.gql.root_resource_id
  path_part   = "graphql"
}

resource "aws_api_gateway_method" "proxy" {
  rest_api_id   = aws_api_gateway_rest_api.gql.id
  resource_id   = aws_api_gateway_resource.gql_method.id
  http_method   = "POST"
  authorization = "NONE"
}

// point api to lambda for execution
resource "aws_api_gateway_integration" "lambda" {
  rest_api_id = aws_api_gateway_rest_api.gql.id
  resource_id = aws_api_gateway_method.proxy.resource_id
  http_method = aws_api_gateway_method.proxy.http_method

  integration_http_method = "POST"
  type                    = "AWS_PROXY"
  uri                     = aws_lambda_function.gql_lambda.invoke_arn
}

resource "aws_api_gateway_integration" "lambda_root" {
  rest_api_id = aws_api_gateway_rest_api.gql.id
  resource_id = aws_api_gateway_method.proxy_root.resource_id
  http_method = aws_api_gateway_method.proxy_root.http_method

  integration_http_method = "POST"
  type                    = "AWS_PROXY"
  uri                     = aws_lambda_function.gql_lambda.invoke_arn
}

resource "aws_api_gateway_deployment" "gql" {
  depends_on = [
    aws_api_gateway_integration.lambda,
    aws_api_gateway_integration.lambda_root,
  ]

  rest_api_id = aws_api_gateway_rest_api.gql.id
  stage_name  = local.stage_name
}

resource "aws_api_gateway_method" "proxy_root" {
  rest_api_id   = aws_api_gateway_rest_api.gql.id
  resource_id   = aws_api_gateway_rest_api.gql.root_resource_id
  http_method   = "ANY"
  authorization = "NONE"
}

// Create a custom domain for api
resource "aws_api_gateway_domain_name" "graphql" {
  certificate_arn = local.graphql_cert_arn
  domain_name     = "graphql.${local.bucket_name}"
}

// This creates an A record in Route 53 for graphql subdomain
resource "aws_route53_record" "graphql" {
  name    = aws_api_gateway_domain_name.graphql.domain_name
  type    = "A"
  zone_id = local.zone_id

  alias {
    evaluate_target_health = true
    name                   = aws_api_gateway_domain_name.graphql.cloudfront_domain_name
    zone_id                = aws_api_gateway_domain_name.graphql.cloudfront_zone_id
  }
}

// this maps the custom domain to the api
resource "aws_api_gateway_base_path_mapping" "example" {
  api_id      = aws_api_gateway_rest_api.gql.id
  stage_name  = local.stage_name
  domain_name = aws_api_gateway_domain_name.graphql.domain_name
}

```
</details>


<details style={{paddingLeft: props.theme.detail.paddingLeft}}>
    <summary><strong>Lambda Terraform:</strong></summary>

```json

// attach policy to role
resource "aws_iam_role_policy_attachment" "sns" {
  role       = aws_iam_role.iam_for_lambda.name
  policy_arn = aws_iam_policy.gql_sns.arn
}

resource "aws_iam_role_policy_attachment" "dynamoDb" {
  role       = aws_iam_role.iam_for_lambda.name
  policy_arn = aws_iam_policy.gql_dynamoDb.arn
}

resource "aws_iam_role_policy_attachment" "basic" {
  policy_arn = "arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole"
  role       = aws_iam_role.iam_for_lambda.name
}

// create policy
resource "aws_iam_policy" "gql_dynamoDb" {
  name        = "${local.resource_prefix}-gql-dynamodb"
  description = "Policy for dynamo db access"
  policy      = data.aws_iam_policy_document.gql_dynamoDb.json
}

// create policy document
data "aws_iam_policy_document" "gql_dynamoDb" {
  statement {
    actions = [
      "dynamodb:BatchGetItem",
      "dynamodb:GetItem",
      "dynamodb:Query",
      "dynamodb:Scan",
      "dynamodb:BatchWriteItem",
      "dynamodb:PutItem",
      "dynamodb:UpdateItem",
      "dynamodb:DescribeTable"
    ]
    effect    = "Allow"
    resources = [local.dynamo_arn, "${local.dynamo_arn}/index/*"]
  }
}

// create policy
resource "aws_iam_policy" "gql_sns" {
  name        = "${local.resource_prefix}-gql-sns"
  description = "Policy for sns publish"
  policy      = data.aws_iam_policy_document.gql_sns.json
}

data "aws_iam_policy_document" "gql_sns" {
  statement {
    actions = [
      "sns:Publish"
    ]
    effect    = "Allow"
    resources = [local.sns_arn]
  }
}


resource "aws_iam_role" "iam_for_lambda" {
  name = "${local.resource_prefix}_iam_for_gql_lambda"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action = "sts:AssumeRole"
        Effect = "Allow"
        Sid    = ""
        Principal = {
          Service = "lambda.amazonaws.com"
        }
      },
    ]
    }
  )

}

// create lambda function
resource "aws_lambda_function" "gql_lambda" {
  filename      = "./function/function.zip"
  function_name = "${local.resource_prefix}_simple_gql_lambda_v"
  role          = aws_iam_role.iam_for_lambda.arn
  handler       = "graphql.graphqlHandler"

  # The filebase64sha256() function is available in Terraform 0.11.12 and later
  # For Terraform 0.11.11 and earlier, use the base64sha256() function and the file() function:
  # source_code_hash = "${base64sha256(file("lambda_function_payload.zip"))}"
  source_code_hash = filebase64sha256("./function/function.zip")

  runtime = "nodejs14.x"

  environment {
    variables = {
      ddb_table_name     = local.dynamo_table_name
      ddb_hash_key       = local.dynamo_hash_key
      ddb_region         = local.region
      GRAPHQL_ENDPOINT   = aws_api_gateway_domain_name.graphql.domain_name
      SNS_ARN            = local.sns_arn
      OKTA_CLIENT_ID     = var.okta_client_id
      OKTA_CLIENT_SECRET = var.okta_client_secret
      OKTA_DOMAIN        = var.okta_domain
      OKTA_ISSUER_SUFFIX = var.okta_issuer_suffix
      OKTA_ISSUER        = local.OKTA_ISSUER
      OKTA_AUDIENCE      = var.okta_audience
      APP_BASE_PORT      = 4000

    }
  }

  tags = merge(
    local.base_tags,
    {
      Name      = "${local.resource_prefix}_simple_gql_lambda_v"
      directory = basename(path.cwd)
    },
  )
}

resource "aws_lambda_permission" "apigw" {
  statement_id  = "AllowAPIGatewayInvoke"
  action        = "lambda:InvokeFunction"
  function_name = aws_lambda_function.gql_lambda.function_name
  principal     = "apigateway.amazonaws.com"

  # The "/*/*" portion grants access from any method on any resource
  # within the API Gateway REST API.
  source_arn = "${aws_api_gateway_rest_api.gql.execution_arn}/*/*"
}

```
</details>
</details>

<br/>

<!-- ######################### Test React and GraphQL ################################## -->

<details style={{paddingLeft: props.theme.detail.paddingLeft}}>
<summary><h2 style={{display: 'inline'}}>Testing React and GraphQL</h2>

We need to build automated testing into our code, even if you start small.  My favorite testing tool is <a href="https://www.cypress.io/ " target="_blank">Cypress.io</a> and I like to use a docker-compose file to set up a local development environment of networked containers that have my local code mounted as directories.  This allows me to continuously build and test with a stable environment that should be the same no matter where I run it.  Cypress provides both a desktop tool, where you can watch tests run in real time and observe the state of the browser as it was at any time during the test, and a command line utility that easily integrates into your favorite pipleline tool.

</summary>

<details style={{paddingLeft: props.theme.detail.paddingLeft}}>
<summary><strong>Compose File</strong></summary>

```yaml
    version: "3.9"  # optional since v1.27.0
    services:
    # UI container - uses command from package.json file to start
    # need to install nodemon
    react:
    image: node:16
    command: /bin/bash -c "ls & npm install nodemon -g && yarn start:nodemon"
    ports:
    - "3001:3001"
    volumes:
    - ./ui:/src
    working_dir: /src
    # This variable is used in App.js, if it exists then it points to this endpoint
    # with docker network the service name (graphql) is all we need to route
    # ui should be running at http://localhost:3001/
    environment:
    DOCKER_GRAPHQL_ENDPOINT: graphql

    dynamodb:
    image: amazon/dynamodb-local:1.17.0
    ports:
    - "8000:8000"
    # gives container a drive to mount
    volumes:
    - ./terraform/terraform_graphql/function/dynamodb:/home/dynamodblocal/data
    working_dir: /home/dynamodblocal
    graphql:
    image: node:16
    command: /bin/bash -c "ls & npm install nodemon -g && nodemon index.js"
    depends_on:
    - dynamodb
    ports:
    - "4000:4000"
    volumes:
    - ./terraform/terraform_graphql/function/src:/src
    - $HOME/.aws/credentials:/root/.aws/credentials
    working_dir: /src
    # graphql should be running at http://localhost:4000/graphql
    environment:
    # This value tells graphql to point at local version of dynamodb if it exists
    CYPRESS_GRAPHQL: "true"
    GRAPHQL_PORT: 4000
    # Tells graphql to ignore the need for authentication
    JWT_OVERRIDE: "true"
    # points to service running above
    DYNAMO_HOST: dynamodb

```

</details>

<br/>


Here the pipeline runs Cypress tests that cover ui and graphql

<Image p={4} verticalAlign='middle' src="/images/6/yaml9.png" />

This part builds the React project and uploads it to S3 bucket:

<Image p={4} verticalAlign='middle' src="/images/6/yaml10.png" />


This conditionally updates the Cloudfront distribution if Terraform files have changed:

<Image p={4} verticalAlign='middle' src="/images/6/yaml11.png" />

</details>

<br/>
<br/>
